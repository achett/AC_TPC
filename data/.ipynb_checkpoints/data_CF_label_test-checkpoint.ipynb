{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changhee/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tensorflow.python.ops.rnn import _transpose_batch_time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#performance metrics\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import normalized_mutual_info_score, homogeneity_score, adjusted_rand_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "#user defined\n",
    "import utils_network as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mode = 'CF_comorbidity_select'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DATASET\n",
    "\n",
    "if data_mode == 'CF':\n",
    "    npz = np.load('./data/CF/data.npz')\n",
    "\n",
    "    data_x        = npz['data_x']\n",
    "    data_y        = npz['data_y']\n",
    "    data_y_onehot = npz['data_y_onehot']\n",
    "    feat_list     = npz['feat_list']\n",
    "    \n",
    "elif data_mode == 'CF_comorbidity':\n",
    "    npz = np.load('./data/CF_comorbidity/data_como.npz')\n",
    "    \n",
    "    data_x        = npz['data_x']\n",
    "    data_y        = npz['data_y']\n",
    "    feat_list     = npz['feat_list']\n",
    "    label_list    = npz['label_list']\n",
    "    selected_list = npz['selected_list']\n",
    "    \n",
    "    data_y_selected = data_y[:, :, np.where([f in selected_list for f in label_list])[0]]\n",
    "    \n",
    "elif data_mode == 'CF_comorbidity_select':\n",
    "    npz = np.load('./data/CF_comorbidity/data_como.npz')\n",
    "    \n",
    "    data_x        = npz['data_x']\n",
    "    data_y        = npz['data_y']\n",
    "    feat_list     = npz['feat_list']\n",
    "    label_list    = npz['label_list']\n",
    "    selected_list = npz['selected_list']\n",
    "    \n",
    "    data_y        = data_y[:, :, np.where([f in selected_list for f in label_list])[0]]\n",
    "    label_list    = selected_list\n",
    "    \n",
    "    tmp_onehot = np.zeros([np.shape(data_y)[0], np.shape(data_y)[1], 8])\n",
    "\n",
    "    tmp_onehot[np.sum(data_y == [0,0,0], axis=2) == 3] = [1, 0, 0, 0, 0, 0, 0, 0]\n",
    "    tmp_onehot[np.sum(data_y == [0,0,1], axis=2) == 3] = [0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    tmp_onehot[np.sum(data_y == [0,1,0], axis=2) == 3] = [0, 0, 1, 0, 0, 0, 0, 0]\n",
    "    tmp_onehot[np.sum(data_y == [0,1,1], axis=2) == 3] = [0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    tmp_onehot[np.sum(data_y == [1,0,0], axis=2) == 3] = [0, 0, 0, 0, 1, 0, 0, 0]\n",
    "    tmp_onehot[np.sum(data_y == [1,0,1], axis=2) == 3] = [0, 0, 0, 0, 0, 1, 0, 0]\n",
    "    tmp_onehot[np.sum(data_y == [1,1,0], axis=2) == 3] = [0, 0, 0, 0, 0, 0, 1, 0]\n",
    "    tmp_onehot[np.sum(data_y == [1,1,1], axis=2) == 3] = [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "    tmp_onehot[np.sum(np.abs(data_x), axis=2) == 0] = [0, 0, 0, 0, 0, 0, 0, 0] #put all 0's for not selected ones..\n",
    "\n",
    "    data_y = tmp_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETER LOGGING\n",
    "def save_logging(dictionary, log_name):\n",
    "    with open(log_name, 'w') as f:\n",
    "        for key, value in dictionary.items():\n",
    "            if 'activate_fn' in key:\n",
    "                value = str(value).split(' ')[1]\n",
    "                \n",
    "            f.write('%s:%s\\n' % (key, value))\n",
    "\n",
    "\n",
    "def load_logging(filename):\n",
    "    data = dict()\n",
    "    with open(filename) as f:\n",
    "        def is_float(input):\n",
    "            try:\n",
    "                num = float(input)\n",
    "            except ValueError:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        for line in f.readlines():\n",
    "            if ':' in line:\n",
    "                key,value = line.strip().split(':', 1)\n",
    "                \n",
    "                if 'activate_fn' in key:\n",
    "                    if value == 'relu':\n",
    "                        value = tf.nn.relu\n",
    "                    elif value == 'elu':\n",
    "                        value = tf.nn.elu\n",
    "                    elif value == 'tanh':\n",
    "                        value = tf.nn.tanh\n",
    "                    else:\n",
    "                        raise ValueError('ERROR: wrong choice of activation function!')\n",
    "                    data[key] = value\n",
    "                else:\n",
    "                    if value.isdigit():\n",
    "                        data[key] = int(value)\n",
    "                    elif is_float(value):\n",
    "                        data[key] = float(value)\n",
    "                    elif value == 'None':\n",
    "                        data[key] = None\n",
    "                    else:\n",
    "                        data[key] = value\n",
    "            else:\n",
    "                pass # deal with bad lines of text here    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(x): \n",
    "    return tf.log(x + 1e-8)\n",
    "\n",
    "def div(x, y):\n",
    "    return tf.div(x, (y + 1e-8))\n",
    "\n",
    "def get_seq_length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "    tmp_length = tf.reduce_sum(used, 1)\n",
    "    tmp_length = tf.cast(tmp_length, tf.int32)\n",
    "    return tmp_length\n",
    "\n",
    "\n",
    "def f_get_minibatch(mb_size, x, y):\n",
    "    idx = range(np.shape(x)[0])\n",
    "    idx = random.sample(idx, mb_size)\n",
    "\n",
    "    x_mb    = x[idx, :, :].astype(float)    \n",
    "    y_mb    = y[idx, :, :].astype(float)    \n",
    "\n",
    "    return x_mb, y_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PERFORMANCE METRICS:\n",
    "def f_get_prediction_scores(y_true_, y_pred_):\n",
    "    if np.sum(y_true_) == 0: #no label for running roc_auc_curves\n",
    "        auroc_ = -1.\n",
    "        auprc_ = -1.\n",
    "    else:\n",
    "        auroc_ = roc_auc_score(y_true_, y_pred_)\n",
    "        auprc_ = average_precision_score(y_true_, y_pred_)\n",
    "    return (auroc_, auprc_)\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    c_matrix = contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(c_matrix, axis=0)) / np.sum(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCN_E2P:\n",
    "    def __init__(self, sess, name, input_dims, network_settings):\n",
    "        self.sess               = sess\n",
    "        self.name               = name\n",
    "        \n",
    "        # INPUT/OUTPUT DIMENSIONS\n",
    "        self.x_dim           = input_dims['x_dim'] #features + delta\n",
    "        self.max_length      = input_dims['max_length']\n",
    "        \n",
    "        self.y_dim           = input_dims['y_dim']\n",
    "        self.y_type          = input_dims['y_type'] #categorical, binary, continuous\n",
    "        \n",
    "\n",
    "        # Encoder\n",
    "        self.h_dim_f         = network_settings['h_dim_encoder'] #encoder nodes\n",
    "        self.num_layers_f    = network_settings['num_layers_encoder'] #encoder layers\n",
    "        self.rnn_type        = network_settings['rnn_type']\n",
    "        self.rnn_activate_fn = network_settings['rnn_activate_fn']\n",
    "\n",
    "        # Predictor\n",
    "        self.h_dim_g         = network_settings['h_dim_predictor'] #predictor nodes\n",
    "        self.num_layers_g    = network_settings['num_layers_predictor'] #predictor layers\n",
    "        \n",
    "        self.fc_activate_fn    = network_settings['fc_activate_fn'] #selector & predictor\n",
    "        \n",
    "        # Latent Space\n",
    "        self.z_dim           = self.h_dim_f * self.num_layers_f\n",
    "\n",
    "        \n",
    "        self._build_net()\n",
    "\n",
    "        \n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            \n",
    "            self.mb_size     = tf.placeholder(tf.int32, [], name='batch_size')\n",
    "            self.lr_rate    = tf.placeholder(tf.float32, name='learning_rate')\n",
    "            self.keep_prob   = tf.placeholder(tf.float32, name='keep_probability')\n",
    "\n",
    "            self.x          = tf.placeholder(tf.float32, [None, self.max_length, self.x_dim], name='inputs')\n",
    "            self.y          = tf.placeholder(tf.float32, [None, self.max_length, self.y_dim], name='labels')\n",
    "#             self.y_onehot   = tf.placeholder(tf.float32, [None, self.max_length, self.y_dim], name='labels_onehot')\n",
    "            \n",
    "            \n",
    "            #FOR CLUSTERING\n",
    "            self.E          = tf.placeholder(tf.float32, [None, self.z_dim], name='embeddings') #mu\n",
    "            self.K          = tf.placeholder(tf.int32, [], name='num_Cluster')\n",
    "            self.s          = tf.placeholder(tf.int32, [None], name='s')\n",
    "            s_one_hot       = tf.one_hot(self.s, self.K, name='s_one_hot')\n",
    "\n",
    "            \n",
    "            # LOSS PARAMETERS\n",
    "            self.alpha      = tf.placeholder(tf.float32, name = 'alpha')\n",
    "\n",
    "    \n",
    "            '''\n",
    "                ### CREATE RNN MASK\n",
    "                    - This is to flexibly handle sequences with different length\n",
    "                    - rnn_mask1: last observation; [mb_size, max_length]\n",
    "                    - rnn_mask2: all available observations; [mb_size, max_length]\n",
    "            '''\n",
    "            # CREATE RNN MASK:            \n",
    "            seq_length     = get_seq_length(self.x)\n",
    "            tmp_range      = tf.expand_dims(tf.range(0, self.max_length, 1), axis=0)\n",
    "            self.rnn_mask1 = tf.cast(tf.equal(tmp_range, tf.expand_dims(seq_length-1, axis=1)), tf.float32) #last observation\n",
    "            self.rnn_mask2 = tf.cast(tf.less_equal(tmp_range, tf.expand_dims(seq_length-1, axis=1)), tf.float32) #all available observation\n",
    "                      \n",
    "            \n",
    "            ### DEFINE PREDICTOR\n",
    "            def predictor(x_, o_dim_=self.y_dim, o_type_=self.y_type, num_layers_=1, h_dim_=self.h_dim_g, activation_fn=self.fc_activate_fn, reuse=tf.AUTO_REUSE):\n",
    "                if o_type_ == 'continuous':\n",
    "                    out_fn = None\n",
    "                elif o_type_ == 'categorical':\n",
    "                    out_fn = tf.nn.softmax #for classification task\n",
    "                elif o_type_ == 'binary':\n",
    "                    out_fn = tf.nn.sigmoid\n",
    "                else:\n",
    "                    raise Exception('Wrong output type. The value {}!!'.format(o_type_))\n",
    "                    \n",
    "                with tf.variable_scope('predictor', reuse=reuse):\n",
    "                    if num_layers_ == 1:\n",
    "                        out =  tf.contrib.layers.fully_connected(inputs=x_, num_outputs=o_dim_, activation_fn=out_fn, scope='predictor_out')\n",
    "                    else: #num_layers > 1\n",
    "                        for tmp_layer in range(num_layers_-1):\n",
    "                            if tmp_layer == 0:\n",
    "                                net = x_\n",
    "                            net = tf.contrib.layers.fully_connected(inputs=net, num_outputs=h_dim_, activation_fn=activation_fn, scope='predictor_'+str(tmp_layer))\n",
    "                            net = tf.nn.dropout(net, keep_prob=self.keep_prob)\n",
    "                        out =  tf.contrib.layers.fully_connected(inputs=net, num_outputs=o_dim_, activation_fn=out_fn, scope='predictor_out')  \n",
    "                return out\n",
    "\n",
    "            \n",
    "            ### DEFINE LOOP FUNCTION FOR ENCODRER (f-g, f-h relations are created here)\n",
    "            def loop_fn(time, cell_output, cell_state, loop_state):\n",
    "                \n",
    "                emit_output = cell_output \n",
    "\n",
    "                if cell_output is None:  # time == 0\n",
    "                    next_cell_state = cell.zero_state(self.mb_size, tf.float32)\n",
    "                    next_loop_state = loop_state_ta\n",
    "                else:\n",
    "                    next_cell_state = cell_state\n",
    "                    tmp_z  = utils.create_concat_state_h(next_cell_state, self.num_layers_f, self.rnn_type)      \n",
    "                    tmp_y  = predictor(tmp_z, self.y_dim, self.y_type, self.num_layers_g, self.h_dim_g, self.fc_activate_fn)        \n",
    "\n",
    "                    next_loop_state = (loop_state[0].write(time-1, tmp_z),  # save all the hidden states\n",
    "                                       loop_state[1].write(time-1, tmp_y)) # save all the selector_net output (i.e., pi)\n",
    "\n",
    "                elements_finished = (time >= max_length)\n",
    "\n",
    "                #this gives the break-point (no more recurrence after the max_length)\n",
    "                finished = tf.reduce_all(elements_finished)    \n",
    "                next_input = tf.cond(finished, \n",
    "                                     lambda: tf.zeros([self.mb_size, self.x_dim], dtype=tf.float32),  \n",
    "                                     lambda: inputs_ta.read(time))\n",
    "                return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "\n",
    "            \n",
    "            '''\n",
    "                ##### CREATE RNN NETWORK\n",
    "                    - (INPUT)  inputs_ta: TensorArray with [max_length, mb_size, x_dim] #x_dim included delta\n",
    "                    - (OUTPUT) \n",
    "                        . zs     = rnn states (h) in LSTM/GRU             ; [mb_size, max_length z_dim]\n",
    "                        . y_hats = output of predictor taking zs as inputs; [mb_size, max_length, y_dim]\n",
    "\n",
    "            '''\n",
    "            inputs    = self.x\n",
    "            inputs_ta = tf.TensorArray(\n",
    "                dtype=tf.float32, size=self.max_length\n",
    "            ).unstack(_transpose_batch_time(inputs), name = 'rnn_input')\n",
    "\n",
    "\n",
    "            cell = utils.create_rnn_cell(\n",
    "                self.h_dim_f, self.num_layers_f, \n",
    "                self.keep_prob, self.rnn_type, self.rnn_activate_fn\n",
    "            )\n",
    "\n",
    "            #define the loop_state TensorArray for information from rnn time steps\n",
    "            loop_state_ta = (\n",
    "                tf.TensorArray(size=self.max_length, dtype=tf.float32, clear_after_read=False),  #zs (j=1,...,J)\n",
    "                tf.TensorArray(size=self.max_length, dtype=tf.float32, clear_after_read=False)  #y_hats (j=1,...,J)\n",
    "            )  \n",
    "\n",
    "            _, _, loop_state_ta = tf.nn.raw_rnn(cell, loop_fn) #, parallel_iterations=1)\n",
    "\n",
    "\n",
    "            self.zs         = _transpose_batch_time(loop_state_ta[0].stack())\n",
    "            self.y_hats     = _transpose_batch_time(loop_state_ta[1].stack())\n",
    "            \n",
    "            #Last latent representation\n",
    "            self. Z         = tf.reduce_sum(tf.tile(tf.expand_dims(self.rnn_mask1, axis=2), [1,1,self.z_dim]) * self.zs, axis=1)\n",
    "                        \n",
    "            \n",
    "            ### DEFINE LOSS FUNCTIONS\n",
    "            #\\ell_{1}: KL divergence loss for regression and binary/categorical-classification task\n",
    "            def loss_1(y_true_, y_pred_, y_type_ = self.y_type):                \n",
    "                if y_type_ == 'continuous':\n",
    "                    tmp_loss = tf.reduce_sum((y_true_ - y_pred_)**2, axis=2)\n",
    "                elif y_type_ == 'categorical':\n",
    "                    tmp_loss = - tf.reduce_sum(y_true_ * log(y_pred_), axis=2)\n",
    "                elif y_type_ == 'binary':\n",
    "                    tmp_loss = - tf.reduce_sum(y_true_ * log(y_pred_) + (1.-y_true_) * log(1.-y_pred_), axis=2)\n",
    "                else:\n",
    "                    raise Exception('Wrong output type. The value {}!!'.format(y_type_))                    \n",
    "                return tmp_loss\n",
    "\n",
    "\n",
    "            ## LOSS_MLE: MLE loss\n",
    "            self.LOSS_MLE   = tf.reduce_mean(tf.reduce_sum(self.rnn_mask2 * loss_1(self.y, self.y_hats, self.y_type), axis=1))\n",
    "\n",
    "            \n",
    "            ### CLUSTER LOSS\n",
    "            Z_expanded      = tf.tile(tf.expand_dims(self.Z, axis=1), [1, self.K, 1])     #[None, num_Cluster, 2]\n",
    "            MU_expanded     = tf.tile(tf.expand_dims(self.E, axis=0), [self.mb_size, 1, 1])        #[None, num_Cluster, 2]\n",
    "            dist_z_expanded = tf.reduce_sum((Z_expanded - MU_expanded)**2, axis=2) #[None, num_Cluster]\n",
    "\n",
    "            dist_z_homo     = tf.reduce_sum(dist_z_expanded * s_one_hot, axis=1) #[None]\n",
    "            dist_z_hetero   = tf.reduce_sum(dist_z_expanded * (1. -s_one_hot), axis=1) #[None]\n",
    "\n",
    "            dist_z_homo     = tf.reduce_mean(dist_z_homo)\n",
    "            dist_z_hetero   = tf.reduce_mean(dist_z_hetero)\n",
    "\n",
    "            self.LOSS_CLU = dist_z_homo\n",
    "\n",
    "            \n",
    "            self.LOSS_TOTAL = self.LOSS_MLE + self.alpha*self.LOSS_CLU\n",
    "\n",
    "\n",
    "            predictor_vars  = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='rnn/predictor')\n",
    "            encoder_vars    = [vars_ for vars_ in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES) \n",
    "                               if vars_ not in predictor_vars]\n",
    "            \n",
    "            \n",
    "            self.solver_MLE           = tf.train.AdamOptimizer(self.lr_rate).minimize(self.LOSS_MLE)#, var_list=encoder_vars+predictor_vars)\n",
    "            self.solver_TOTAL         = tf.train.AdamOptimizer(self.lr_rate).minimize(self.LOSS_TOTAL)\n",
    "\n",
    "\n",
    "            #to check the predictor output given z\n",
    "            self.zz     = tf.placeholder(tf.float32, [None, self.z_dim])\n",
    "            with tf.variable_scope('rnn', reuse=True):\n",
    "                self.yy   = predictor(self.zz, self.y_dim, self.y_type, self.num_layers_g, self.h_dim_g, self.fc_activate_fn)\n",
    "                \n",
    "\n",
    "    def train_mle(self, x_, y_, lr_train, k_prob):\n",
    "        return self.sess.run([self.solver_MLE, self.LOSS_MLE],\n",
    "                             feed_dict={self.x: x_, self.y: y_, \n",
    "                                        self.mb_size:np.shape(x_)[0], self.lr_rate: lr_train, self.keep_prob: k_prob})\n",
    "    \n",
    "    def train_total(self, x_, y_, s_, E_, K_, a, lr_train, k_prob):\n",
    "        return self.sess.run([self.solver_TOTAL, self.LOSS_TOTAL, self.LOSS_MLE, self.LOSS_CLU],\n",
    "                             feed_dict={self.x: x_, self.y: y_, self.s: s_, self.E: E_, self.K: K_,\n",
    "                                        self.mb_size:np.shape(x_)[0], self.alpha:a,\n",
    "                                        self.lr_rate: lr_train, self.keep_prob: k_prob})\n",
    "    \n",
    "    def predict_y_hats(self, x_):\n",
    "        return self.sess.run([self.y_hats, self.rnn_mask2], \n",
    "                             feed_dict={self.x:x_, self.mb_size:np.shape(x_)[0], self.keep_prob:1.0})\n",
    "    \n",
    "    def predict_zs(self, x_):\n",
    "        return self.sess.run([self.zs, self.rnn_mask2], \n",
    "                             feed_dict={self.x:x_, self.mb_size:np.shape(x_)[0], self.keep_prob:1.0})\n",
    "    \n",
    "    def predict_Z(self, x_): #get the last latent encoding\n",
    "        return self.sess.run(self.Z, \n",
    "                             feed_dict={self.x:x_, self.mb_size:np.shape(x_)[0], self.keep_prob:1.0})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dim_FC   = 50 #for fully_connected layers\n",
    "h_dim_RNN  = 50\n",
    "\n",
    "x_dim = np.shape(data_x)[2]\n",
    "y_dim = np.shape(data_y)[2]\n",
    "\n",
    "if data_mode == 'CF':\n",
    "    y_type = 'categorical'\n",
    "elif data_mode == 'CF_comorbidity':\n",
    "    y_type = 'binary'\n",
    "elif data_mode == 'CF_comorbidity_select':\n",
    "    y_type = 'categorical'\n",
    "    \n",
    "num_layer_encoder    = 1\n",
    "num_layer_predictor  = 2\n",
    "\n",
    "z_dim = h_dim_RNN * num_layer_encoder\n",
    "\n",
    "max_length = np.shape(data_x)[1]\n",
    "\n",
    "rnn_type          = 'LSTM' #GRU, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims ={\n",
    "    'x_dim': x_dim,\n",
    "    'y_dim': y_dim,\n",
    "    'max_length': max_length,\n",
    "    'y_type': y_type\n",
    "}\n",
    "\n",
    "network_settings ={\n",
    "    'h_dim_encoder': h_dim_RNN,\n",
    "    'num_layers_encoder': num_layer_encoder,\n",
    "    'rnn_type': rnn_type,\n",
    "    'rnn_activate_fn': tf.nn.tanh,\n",
    "    \n",
    "    'h_dim_predictor': h_dim_FC,\n",
    "    'num_layers_predictor': num_layer_predictor,\n",
    "    \n",
    "    'fc_activate_fn': tf.nn.relu\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_ITERATION = 5\n",
    "\n",
    "RESULT_NMI    = np.zeros([OUT_ITERATION, 1])\n",
    "RESULT_RI     = np.zeros([OUT_ITERATION, 1])\n",
    "RESULT_PURITY = np.zeros([OUT_ITERATION, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Turn on xla optimization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "model = DCN_E2P(sess, \"dcn_E2P\", input_dims, network_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "out_itr = 0\n",
    "\n",
    "if data_mode == 'CF':\n",
    "    tr_data_x,te_data_x, tr_data_y,te_data_y, tr_data_y_onehot,te_data_y_onehot = train_test_split(\n",
    "        data_x, data_y, data_y_onehot, test_size=0.2, random_state=seed+out_itr\n",
    "    )\n",
    "\n",
    "    tr_data_x,va_data_x, tr_data_y,va_data_y, tr_data_y_onehot,va_data_y_onehot = train_test_split(\n",
    "        tr_data_x, tr_data_y, tr_data_y_onehot, test_size=0.2, random_state=seed+out_itr\n",
    "    )\n",
    "elif data_mode == 'CF_comorbidity':\n",
    "    tr_data_x,te_data_x, tr_data_y,te_data_y = train_test_split(\n",
    "        data_x, data_y, test_size=0.2, random_state=seed+out_itr\n",
    "    )\n",
    "\n",
    "    tr_data_x,va_data_x, tr_data_y,va_data_y = train_test_split(\n",
    "        tr_data_x, tr_data_y, test_size=0.2, random_state=seed+out_itr\n",
    "    )\n",
    "elif data_mode == 'CF_comorbidity_select':\n",
    "    tr_data_x,te_data_x, tr_data_y,te_data_y = train_test_split(\n",
    "        data_x, data_y, test_size=0.2, random_state=seed+out_itr\n",
    "    )\n",
    "\n",
    "    tr_data_x,va_data_x, tr_data_y,va_data_y = train_test_split(\n",
    "        tr_data_x, tr_data_y, test_size=0.2, random_state=seed+out_itr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './{}/dcn_E2P/init/itr{}/'.format(data_mode, out_itr)\n",
    "\n",
    "if not os.path.exists(save_path + '/models/'):\n",
    "    os.makedirs(save_path + '/models/')\n",
    "\n",
    "if not os.path.exists(save_path + '/results/'):\n",
    "    os.makedirs(save_path + '/results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "lr_rate    = 1e-3\n",
    "keep_prob  = 0.7\n",
    "mb_size    = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITR 1000: loss_2=3.5328 | va_auroc:0.9279, va_auprc:0.6123\n",
      "ITR 2000: loss_2=2.6224 | va_auroc:0.9166, va_auprc:0.5837\n",
      "ITR 3000: loss_2=2.3386 | va_auroc:0.9043, va_auprc:0.5518\n",
      "ITR 4000: loss_2=2.1690 | va_auroc:0.9009, va_auprc:0.5197\n",
      "ITR 5000: loss_2=2.0748 | va_auroc:0.8936, va_auprc:0.5056\n",
      "ITR 6000: loss_2=1.9900 | va_auroc:0.8936, va_auprc:0.5074\n",
      "ITR 7000: loss_2=1.9322 | va_auroc:0.8847, va_auprc:0.4933\n",
      "ITR 8000: loss_2=1.8913 | va_auroc:0.8867, va_auprc:0.4971\n",
      "ITR 9000: loss_2=1.8549 | va_auroc:0.8805, va_auprc:0.4870\n",
      "ITR 10000: loss_2=1.8282 | va_auroc:0.8835, va_auprc:0.5035\n",
      "ITR 11000: loss_2=1.7946 | va_auroc:0.8823, va_auprc:0.4938\n",
      "ITR 12000: loss_2=1.7767 | va_auroc:0.8761, va_auprc:0.4856\n",
      "ITR 13000: loss_2=1.7539 | va_auroc:0.8762, va_auprc:0.4638\n",
      "ITR 14000: loss_2=1.7354 | va_auroc:0.8791, va_auprc:0.4774\n",
      "ITR 15000: loss_2=1.7241 | va_auroc:0.8754, va_auprc:0.4709\n",
      "ITR 16000: loss_2=1.6905 | va_auroc:0.8758, va_auprc:0.4707\n",
      "ITR 17000: loss_2=1.6871 | va_auroc:0.8796, va_auprc:0.4719\n",
      "ITR 18000: loss_2=1.6852 | va_auroc:0.8741, va_auprc:0.4515\n",
      "ITR 19000: loss_2=1.6575 | va_auroc:0.8732, va_auprc:0.4719\n",
      "ITR 20000: loss_2=1.6598 | va_auroc:0.8746, va_auprc:0.4625\n",
      "ITR 21000: loss_2=1.6511 | va_auroc:0.8707, va_auprc:0.4541\n",
      "ITR 22000: loss_2=1.6342 | va_auroc:0.8735, va_auprc:0.4585\n",
      "ITR 23000: loss_2=1.6347 | va_auroc:0.8770, va_auprc:0.4648\n",
      "ITR 24000: loss_2=1.6347 | va_auroc:0.8744, va_auprc:0.4617\n",
      "ITR 25000: loss_2=1.6184 | va_auroc:0.8693, va_auprc:0.4490\n",
      "ITR 26000: loss_2=1.6181 | va_auroc:0.8764, va_auprc:0.4507\n",
      "ITR 27000: loss_2=1.6066 | va_auroc:0.8744, va_auprc:0.4605\n",
      "ITR 28000: loss_2=1.5930 | va_auroc:0.8729, va_auprc:0.4562\n",
      "ITR 29000: loss_2=1.6006 | va_auroc:0.8705, va_auprc:0.4469\n",
      "ITR 30000: loss_2=1.5888 | va_auroc:0.8750, va_auprc:0.4492\n",
      "ITR 31000: loss_2=1.5846 | va_auroc:0.8721, va_auprc:0.4579\n",
      "ITR 32000: loss_2=1.5676 | va_auroc:0.8753, va_auprc:0.4453\n",
      "ITR 33000: loss_2=1.5869 | va_auroc:0.8737, va_auprc:0.4565\n",
      "ITR 34000: loss_2=1.5643 | va_auroc:0.8724, va_auprc:0.4536\n",
      "ITR 35000: loss_2=1.5635 | va_auroc:0.8689, va_auprc:0.4464\n",
      "ITR 36000: loss_2=1.5698 | va_auroc:0.8747, va_auprc:0.4577\n",
      "ITR 37000: loss_2=1.5441 | va_auroc:0.8728, va_auprc:0.4470\n",
      "ITR 38000: loss_2=1.5563 | va_auroc:0.8710, va_auprc:0.4571\n",
      "ITR 39000: loss_2=1.5515 | va_auroc:0.8713, va_auprc:0.4518\n",
      "ITR 40000: loss_2=1.5482 | va_auroc:0.8683, va_auprc:0.4541\n",
      "ITR 41000: loss_2=1.5550 | va_auroc:0.8655, va_auprc:0.4526\n",
      "ITR 42000: loss_2=1.5374 | va_auroc:0.8684, va_auprc:0.4468\n",
      "ITR 43000: loss_2=1.5344 | va_auroc:0.8692, va_auprc:0.4556\n",
      "ITR 44000: loss_2=1.5276 | va_auroc:0.8694, va_auprc:0.4450\n",
      "ITR 45000: loss_2=1.5276 | va_auroc:0.8631, va_auprc:0.4472\n",
      "ITR 46000: loss_2=1.5258 | va_auroc:0.8711, va_auprc:0.4713\n",
      "ITR 47000: loss_2=1.5191 | va_auroc:0.8708, va_auprc:0.4389\n",
      "ITR 48000: loss_2=1.5156 | va_auroc:0.8719, va_auprc:0.4431\n",
      "ITR 49000: loss_2=1.5157 | va_auroc:0.8702, va_auprc:0.4353\n",
      "ITR 50000: loss_2=1.5166 | va_auroc:0.8742, va_auprc:0.4408\n"
     ]
    }
   ],
   "source": [
    "ITERATION  = 50000\n",
    "check_step = 1000\n",
    "\n",
    "avg_loss_mle  = 0\n",
    "for itr in range(ITERATION):\n",
    "    x_mb, y_mb = f_get_minibatch(mb_size, tr_data_x, tr_data_y)\n",
    "    \n",
    "    _, tmp_loss_mle= model.train_mle(x_mb, y_mb, lr_rate, keep_prob)\n",
    "    avg_loss_mle += tmp_loss_mle/check_step\n",
    "    \n",
    "    if (itr+1)%check_step == 0:                \n",
    "        tmp_y, tmp_m = model.predict_y_hats(va_data_x)\n",
    "\n",
    "        y_pred = tmp_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
    "        y_true = va_data_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
    "        \n",
    "        AUROC = np.zeros([y_dim])\n",
    "        AUPRC = np.zeros([y_dim])\n",
    "        for y_idx in range(y_dim):\n",
    "            auroc, auprc = f_get_prediction_scores(y_true[:, y_idx], y_pred[:, y_idx])\n",
    "            AUROC[y_idx] = auroc\n",
    "            AUPRC[y_idx] = auprc\n",
    "            \n",
    "        print (\"ITR {}: loss_2={:.4f} | va_auroc:{:.4f}, va_auprc:{:.4f}\".format(\n",
    "            itr+1, avg_loss_mle, np.mean(AUROC), np.mean(AUPRC)))\n",
    "        \n",
    "        avg_loss_mle = 0\n",
    "        \n",
    "saver.save(sess, save_path + 'models/dcn_E2P_v4_init')\n",
    "save_logging(network_settings, save_path + 'models/network_settings_v4.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.save(sess, save_path + 'models/dcn_E2P_v4_init')\n",
    "save_logging(network_settings, save_path + 'models/network_settings_v4.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITR 1000: loss_2=3.5245 | va_auroc:0.9311, va_auprc:0.6494\n",
      "ITR 2000: loss_2=2.6116 | va_auroc:0.9184, va_auprc:0.5977\n",
      "ITR 3000: loss_2=2.3233 | va_auroc:0.9109, va_auprc:0.5550\n",
      "ITR 4000: loss_2=2.1541 | va_auroc:0.9054, va_auprc:0.5346\n",
      "ITR 5000: loss_2=2.0763 | va_auroc:0.9026, va_auprc:0.5227\n",
      "ITR 6000: loss_2=1.9935 | va_auroc:0.9002, va_auprc:0.5143\n",
      "ITR 7000: loss_2=1.9401 | va_auroc:0.8964, va_auprc:0.5248\n",
      "ITR 8000: loss_2=1.8791 | va_auroc:0.8955, va_auprc:0.5254\n",
      "ITR 9000: loss_2=1.8576 | va_auroc:0.8891, va_auprc:0.5125\n",
      "ITR 10000: loss_2=1.8172 | va_auroc:0.8964, va_auprc:0.5218\n",
      "ITR 11000: loss_2=1.7970 | va_auroc:0.8939, va_auprc:0.5229\n",
      "ITR 12000: loss_2=1.7709 | va_auroc:0.8880, va_auprc:0.5121\n",
      "ITR 13000: loss_2=1.7556 | va_auroc:0.8853, va_auprc:0.5068\n",
      "ITR 14000: loss_2=1.7356 | va_auroc:0.8873, va_auprc:0.5095\n",
      "ITR 15000: loss_2=1.7163 | va_auroc:0.8852, va_auprc:0.5078\n",
      "ITR 16000: loss_2=1.7007 | va_auroc:0.8852, va_auprc:0.5111\n",
      "ITR 17000: loss_2=1.7022 | va_auroc:0.8896, va_auprc:0.5107\n",
      "ITR 18000: loss_2=1.6834 | va_auroc:0.8828, va_auprc:0.4972\n",
      "ITR 19000: loss_2=1.6780 | va_auroc:0.8856, va_auprc:0.5098\n",
      "ITR 20000: loss_2=1.6549 | va_auroc:0.8851, va_auprc:0.4990\n",
      "ITR 21000: loss_2=1.6408 | va_auroc:0.8816, va_auprc:0.4817\n",
      "ITR 22000: loss_2=1.6410 | va_auroc:0.8834, va_auprc:0.4856\n",
      "ITR 23000: loss_2=1.6340 | va_auroc:0.8811, va_auprc:0.4889\n",
      "ITR 24000: loss_2=1.6170 | va_auroc:0.8830, va_auprc:0.4829\n",
      "ITR 25000: loss_2=1.6113 | va_auroc:0.8834, va_auprc:0.4995\n",
      "ITR 26000: loss_2=1.6056 | va_auroc:0.8790, va_auprc:0.4820\n",
      "ITR 27000: loss_2=1.6063 | va_auroc:0.8828, va_auprc:0.4912\n",
      "ITR 28000: loss_2=1.6019 | va_auroc:0.8809, va_auprc:0.4821\n",
      "ITR 29000: loss_2=1.5866 | va_auroc:0.8768, va_auprc:0.4759\n",
      "ITR 30000: loss_2=1.5858 | va_auroc:0.8807, va_auprc:0.4895\n",
      "ITR 31000: loss_2=1.5838 | va_auroc:0.8782, va_auprc:0.4767\n",
      "ITR 32000: loss_2=1.5748 | va_auroc:0.8825, va_auprc:0.4899\n",
      "ITR 33000: loss_2=1.5683 | va_auroc:0.8799, va_auprc:0.4902\n",
      "ITR 34000: loss_2=1.5599 | va_auroc:0.8817, va_auprc:0.4912\n",
      "ITR 35000: loss_2=1.5589 | va_auroc:0.8821, va_auprc:0.4910\n",
      "ITR 36000: loss_2=1.5601 | va_auroc:0.8766, va_auprc:0.4963\n",
      "ITR 37000: loss_2=1.5551 | va_auroc:0.8843, va_auprc:0.4889\n",
      "ITR 38000: loss_2=1.5559 | va_auroc:0.8834, va_auprc:0.4984\n",
      "ITR 39000: loss_2=1.5486 | va_auroc:0.8790, va_auprc:0.4892\n",
      "ITR 40000: loss_2=1.5447 | va_auroc:0.8828, va_auprc:0.4879\n",
      "ITR 41000: loss_2=1.5327 | va_auroc:0.8802, va_auprc:0.4906\n",
      "ITR 42000: loss_2=1.5294 | va_auroc:0.8830, va_auprc:0.4913\n",
      "ITR 43000: loss_2=1.5213 | va_auroc:0.8799, va_auprc:0.4933\n",
      "ITR 44000: loss_2=1.5217 | va_auroc:0.8827, va_auprc:0.4931\n",
      "ITR 45000: loss_2=1.5290 | va_auroc:0.8793, va_auprc:0.4770\n",
      "ITR 46000: loss_2=1.5256 | va_auroc:0.8826, va_auprc:0.5004\n",
      "ITR 47000: loss_2=1.5184 | va_auroc:0.8804, va_auprc:0.5064\n",
      "ITR 48000: loss_2=1.5211 | va_auroc:0.8816, va_auprc:0.5102\n",
      "ITR 49000: loss_2=1.5114 | va_auroc:0.8769, va_auprc:0.5071\n",
      "ITR 50000: loss_2=1.5110 | va_auroc:0.8810, va_auprc:0.4960\n",
      "ITR 1000: loss_2=3.4658 | va_auroc:0.9241, va_auprc:0.6109\n",
      "ITR 2000: loss_2=2.6006 | va_auroc:0.9063, va_auprc:0.5904\n",
      "ITR 3000: loss_2=2.3193 | va_auroc:0.8947, va_auprc:0.5615\n",
      "ITR 4000: loss_2=2.1569 | va_auroc:0.8891, va_auprc:0.5437\n",
      "ITR 5000: loss_2=2.0446 | va_auroc:0.8818, va_auprc:0.5324\n",
      "ITR 6000: loss_2=1.9652 | va_auroc:0.8813, va_auprc:0.5220\n",
      "ITR 7000: loss_2=1.9079 | va_auroc:0.8790, va_auprc:0.5098\n",
      "ITR 8000: loss_2=1.8619 | va_auroc:0.8733, va_auprc:0.5200\n",
      "ITR 9000: loss_2=1.8345 | va_auroc:0.8792, va_auprc:0.5103\n",
      "ITR 10000: loss_2=1.8015 | va_auroc:0.8762, va_auprc:0.5093\n",
      "ITR 11000: loss_2=1.7780 | va_auroc:0.8777, va_auprc:0.5032\n",
      "ITR 12000: loss_2=1.7572 | va_auroc:0.8772, va_auprc:0.5113\n",
      "ITR 13000: loss_2=1.7303 | va_auroc:0.8756, va_auprc:0.5057\n",
      "ITR 14000: loss_2=1.7152 | va_auroc:0.8783, va_auprc:0.5128\n",
      "ITR 15000: loss_2=1.7008 | va_auroc:0.8740, va_auprc:0.5142\n",
      "ITR 16000: loss_2=1.6972 | va_auroc:0.8730, va_auprc:0.4910\n",
      "ITR 17000: loss_2=1.6738 | va_auroc:0.8758, va_auprc:0.4978\n",
      "ITR 18000: loss_2=1.6645 | va_auroc:0.8736, va_auprc:0.4979\n",
      "ITR 19000: loss_2=1.6589 | va_auroc:0.8706, va_auprc:0.4934\n",
      "ITR 20000: loss_2=1.6325 | va_auroc:0.8686, va_auprc:0.4873\n",
      "ITR 21000: loss_2=1.6200 | va_auroc:0.8715, va_auprc:0.4902\n",
      "ITR 22000: loss_2=1.6203 | va_auroc:0.8684, va_auprc:0.4981\n",
      "ITR 23000: loss_2=1.6138 | va_auroc:0.8674, va_auprc:0.4931\n",
      "ITR 24000: loss_2=1.5979 | va_auroc:0.8746, va_auprc:0.4923\n",
      "ITR 25000: loss_2=1.6038 | va_auroc:0.8706, va_auprc:0.4839\n",
      "ITR 26000: loss_2=1.5728 | va_auroc:0.8715, va_auprc:0.4920\n",
      "ITR 27000: loss_2=1.5782 | va_auroc:0.8695, va_auprc:0.4984\n",
      "ITR 28000: loss_2=1.5828 | va_auroc:0.8723, va_auprc:0.4887\n",
      "ITR 29000: loss_2=1.5794 | va_auroc:0.8722, va_auprc:0.4981\n",
      "ITR 30000: loss_2=1.5725 | va_auroc:0.8691, va_auprc:0.4866\n",
      "ITR 31000: loss_2=1.5688 | va_auroc:0.8715, va_auprc:0.4862\n",
      "ITR 32000: loss_2=1.5574 | va_auroc:0.8661, va_auprc:0.4813\n",
      "ITR 33000: loss_2=1.5462 | va_auroc:0.8704, va_auprc:0.4822\n",
      "ITR 34000: loss_2=1.5564 | va_auroc:0.8687, va_auprc:0.4649\n",
      "ITR 35000: loss_2=1.5571 | va_auroc:0.8743, va_auprc:0.4935\n",
      "ITR 36000: loss_2=1.5345 | va_auroc:0.8685, va_auprc:0.4770\n",
      "ITR 37000: loss_2=1.5438 | va_auroc:0.8681, va_auprc:0.4761\n",
      "ITR 38000: loss_2=1.5357 | va_auroc:0.8722, va_auprc:0.4795\n",
      "ITR 39000: loss_2=1.5393 | va_auroc:0.8676, va_auprc:0.4699\n",
      "ITR 40000: loss_2=1.5255 | va_auroc:0.8657, va_auprc:0.4659\n",
      "ITR 41000: loss_2=1.5277 | va_auroc:0.8660, va_auprc:0.4723\n",
      "ITR 42000: loss_2=1.5172 | va_auroc:0.8697, va_auprc:0.4717\n",
      "ITR 43000: loss_2=1.5105 | va_auroc:0.8741, va_auprc:0.4755\n",
      "ITR 44000: loss_2=1.5113 | va_auroc:0.8719, va_auprc:0.4668\n",
      "ITR 45000: loss_2=1.5234 | va_auroc:0.8684, va_auprc:0.4663\n",
      "ITR 46000: loss_2=1.4995 | va_auroc:0.8663, va_auprc:0.4624\n",
      "ITR 47000: loss_2=1.5016 | va_auroc:0.8676, va_auprc:0.4654\n",
      "ITR 48000: loss_2=1.5019 | va_auroc:0.8668, va_auprc:0.4664\n",
      "ITR 49000: loss_2=1.5129 | va_auroc:0.8637, va_auprc:0.4680\n",
      "ITR 50000: loss_2=1.4870 | va_auroc:0.8662, va_auprc:0.4724\n",
      "ITR 1000: loss_2=3.5118 | va_auroc:0.9310, va_auprc:0.6386\n",
      "ITR 2000: loss_2=2.6017 | va_auroc:0.9168, va_auprc:0.6050\n",
      "ITR 3000: loss_2=2.3373 | va_auroc:0.8979, va_auprc:0.5698\n",
      "ITR 4000: loss_2=2.1746 | va_auroc:0.8895, va_auprc:0.5385\n",
      "ITR 5000: loss_2=2.0619 | va_auroc:0.8908, va_auprc:0.5349\n",
      "ITR 6000: loss_2=1.9888 | va_auroc:0.8893, va_auprc:0.5185\n",
      "ITR 7000: loss_2=1.9373 | va_auroc:0.8840, va_auprc:0.5141\n",
      "ITR 8000: loss_2=1.8846 | va_auroc:0.8835, va_auprc:0.5088\n",
      "ITR 9000: loss_2=1.8518 | va_auroc:0.8819, va_auprc:0.5050\n",
      "ITR 10000: loss_2=1.8139 | va_auroc:0.8826, va_auprc:0.4974\n",
      "ITR 11000: loss_2=1.7914 | va_auroc:0.8834, va_auprc:0.5079\n",
      "ITR 12000: loss_2=1.7672 | va_auroc:0.8793, va_auprc:0.4972\n",
      "ITR 13000: loss_2=1.7509 | va_auroc:0.8775, va_auprc:0.4934\n",
      "ITR 14000: loss_2=1.7149 | va_auroc:0.8789, va_auprc:0.4959\n",
      "ITR 15000: loss_2=1.7047 | va_auroc:0.8789, va_auprc:0.5020\n",
      "ITR 16000: loss_2=1.6890 | va_auroc:0.8788, va_auprc:0.4926\n",
      "ITR 17000: loss_2=1.6679 | va_auroc:0.8790, va_auprc:0.5085\n",
      "ITR 18000: loss_2=1.6608 | va_auroc:0.8798, va_auprc:0.4931\n",
      "ITR 19000: loss_2=1.6507 | va_auroc:0.8795, va_auprc:0.4962\n",
      "ITR 20000: loss_2=1.6408 | va_auroc:0.8802, va_auprc:0.4918\n",
      "ITR 21000: loss_2=1.6402 | va_auroc:0.8820, va_auprc:0.4910\n",
      "ITR 22000: loss_2=1.6303 | va_auroc:0.8788, va_auprc:0.4987\n",
      "ITR 23000: loss_2=1.6231 | va_auroc:0.8800, va_auprc:0.4923\n",
      "ITR 24000: loss_2=1.6168 | va_auroc:0.8809, va_auprc:0.4933\n",
      "ITR 25000: loss_2=1.6040 | va_auroc:0.8812, va_auprc:0.4962\n",
      "ITR 26000: loss_2=1.5982 | va_auroc:0.8762, va_auprc:0.4902\n",
      "ITR 27000: loss_2=1.5889 | va_auroc:0.8788, va_auprc:0.4916\n",
      "ITR 28000: loss_2=1.5876 | va_auroc:0.8750, va_auprc:0.4827\n",
      "ITR 29000: loss_2=1.5744 | va_auroc:0.8804, va_auprc:0.4904\n",
      "ITR 30000: loss_2=1.5681 | va_auroc:0.8775, va_auprc:0.4807\n",
      "ITR 31000: loss_2=1.5686 | va_auroc:0.8764, va_auprc:0.4893\n",
      "ITR 32000: loss_2=1.5761 | va_auroc:0.8790, va_auprc:0.4850\n",
      "ITR 33000: loss_2=1.5661 | va_auroc:0.8766, va_auprc:0.4842\n",
      "ITR 34000: loss_2=1.5493 | va_auroc:0.8740, va_auprc:0.4839\n",
      "ITR 35000: loss_2=1.5572 | va_auroc:0.8751, va_auprc:0.4843\n",
      "ITR 36000: loss_2=1.5449 | va_auroc:0.8759, va_auprc:0.4814\n",
      "ITR 37000: loss_2=1.5390 | va_auroc:0.8815, va_auprc:0.4840\n",
      "ITR 38000: loss_2=1.5395 | va_auroc:0.8786, va_auprc:0.4838\n",
      "ITR 39000: loss_2=1.5346 | va_auroc:0.8741, va_auprc:0.4908\n",
      "ITR 40000: loss_2=1.5251 | va_auroc:0.8768, va_auprc:0.4750\n",
      "ITR 41000: loss_2=1.5179 | va_auroc:0.8770, va_auprc:0.4772\n",
      "ITR 42000: loss_2=1.5127 | va_auroc:0.8735, va_auprc:0.4757\n",
      "ITR 43000: loss_2=1.5080 | va_auroc:0.8732, va_auprc:0.4699\n",
      "ITR 44000: loss_2=1.5154 | va_auroc:0.8781, va_auprc:0.4531\n",
      "ITR 45000: loss_2=1.5186 | va_auroc:0.8768, va_auprc:0.4750\n",
      "ITR 46000: loss_2=1.5105 | va_auroc:0.8767, va_auprc:0.4666\n",
      "ITR 47000: loss_2=1.5053 | va_auroc:0.8770, va_auprc:0.4630\n",
      "ITR 48000: loss_2=1.4967 | va_auroc:0.8739, va_auprc:0.4555\n",
      "ITR 49000: loss_2=1.5011 | va_auroc:0.8746, va_auprc:0.4678\n",
      "ITR 50000: loss_2=1.4941 | va_auroc:0.8750, va_auprc:0.4746\n",
      "ITR 1000: loss_2=3.5160 | va_auroc:0.9221, va_auprc:0.6304\n",
      "ITR 2000: loss_2=2.6026 | va_auroc:0.9088, va_auprc:0.6007\n",
      "ITR 3000: loss_2=2.3290 | va_auroc:0.8953, va_auprc:0.5442\n",
      "ITR 4000: loss_2=2.1684 | va_auroc:0.8865, va_auprc:0.5193\n",
      "ITR 5000: loss_2=2.0688 | va_auroc:0.8785, va_auprc:0.5072\n",
      "ITR 6000: loss_2=1.9846 | va_auroc:0.8815, va_auprc:0.5051\n",
      "ITR 7000: loss_2=1.9214 | va_auroc:0.8764, va_auprc:0.4876\n",
      "ITR 8000: loss_2=1.8809 | va_auroc:0.8725, va_auprc:0.4908\n",
      "ITR 9000: loss_2=1.8416 | va_auroc:0.8687, va_auprc:0.4773\n",
      "ITR 10000: loss_2=1.8065 | va_auroc:0.8717, va_auprc:0.4846\n",
      "ITR 11000: loss_2=1.7732 | va_auroc:0.8664, va_auprc:0.4786\n",
      "ITR 12000: loss_2=1.7691 | va_auroc:0.8677, va_auprc:0.4613\n",
      "ITR 13000: loss_2=1.7464 | va_auroc:0.8643, va_auprc:0.4740\n",
      "ITR 14000: loss_2=1.7205 | va_auroc:0.8618, va_auprc:0.4594\n",
      "ITR 15000: loss_2=1.7090 | va_auroc:0.8566, va_auprc:0.4508\n",
      "ITR 16000: loss_2=1.6951 | va_auroc:0.8628, va_auprc:0.4688\n",
      "ITR 17000: loss_2=1.6730 | va_auroc:0.8686, va_auprc:0.4687\n",
      "ITR 18000: loss_2=1.6770 | va_auroc:0.8629, va_auprc:0.4654\n",
      "ITR 19000: loss_2=1.6637 | va_auroc:0.8640, va_auprc:0.4749\n",
      "ITR 20000: loss_2=1.6473 | va_auroc:0.8630, va_auprc:0.4570\n",
      "ITR 21000: loss_2=1.6310 | va_auroc:0.8624, va_auprc:0.4485\n",
      "ITR 22000: loss_2=1.6275 | va_auroc:0.8624, va_auprc:0.4604\n",
      "ITR 23000: loss_2=1.6234 | va_auroc:0.8567, va_auprc:0.4475\n",
      "ITR 24000: loss_2=1.6138 | va_auroc:0.8624, va_auprc:0.4557\n",
      "ITR 25000: loss_2=1.6149 | va_auroc:0.8577, va_auprc:0.4467\n",
      "ITR 26000: loss_2=1.6001 | va_auroc:0.8572, va_auprc:0.4500\n",
      "ITR 27000: loss_2=1.5999 | va_auroc:0.8575, va_auprc:0.4404\n",
      "ITR 28000: loss_2=1.5946 | va_auroc:0.8550, va_auprc:0.4498\n",
      "ITR 29000: loss_2=1.5891 | va_auroc:0.8562, va_auprc:0.4601\n",
      "ITR 30000: loss_2=1.5811 | va_auroc:0.8548, va_auprc:0.4616\n",
      "ITR 31000: loss_2=1.5746 | va_auroc:0.8576, va_auprc:0.4523\n",
      "ITR 32000: loss_2=1.5697 | va_auroc:0.8547, va_auprc:0.4610\n",
      "ITR 33000: loss_2=1.5716 | va_auroc:0.8567, va_auprc:0.4579\n",
      "ITR 34000: loss_2=1.5626 | va_auroc:0.8590, va_auprc:0.4603\n",
      "ITR 35000: loss_2=1.5521 | va_auroc:0.8549, va_auprc:0.4623\n",
      "ITR 36000: loss_2=1.5513 | va_auroc:0.8536, va_auprc:0.4623\n",
      "ITR 37000: loss_2=1.5433 | va_auroc:0.8587, va_auprc:0.4517\n",
      "ITR 38000: loss_2=1.5367 | va_auroc:0.8594, va_auprc:0.4452\n",
      "ITR 39000: loss_2=1.5408 | va_auroc:0.8602, va_auprc:0.4615\n",
      "ITR 40000: loss_2=1.5303 | va_auroc:0.8559, va_auprc:0.4609\n",
      "ITR 41000: loss_2=1.5236 | va_auroc:0.8548, va_auprc:0.4618\n",
      "ITR 42000: loss_2=1.5283 | va_auroc:0.8619, va_auprc:0.4504\n",
      "ITR 43000: loss_2=1.5246 | va_auroc:0.8546, va_auprc:0.4739\n",
      "ITR 44000: loss_2=1.5137 | va_auroc:0.8588, va_auprc:0.4611\n",
      "ITR 45000: loss_2=1.5179 | va_auroc:0.8611, va_auprc:0.4679\n",
      "ITR 46000: loss_2=1.5101 | va_auroc:0.8590, va_auprc:0.4520\n",
      "ITR 47000: loss_2=1.5140 | va_auroc:0.8621, va_auprc:0.4667\n",
      "ITR 48000: loss_2=1.5159 | va_auroc:0.8531, va_auprc:0.4598\n",
      "ITR 49000: loss_2=1.5199 | va_auroc:0.8588, va_auprc:0.4743\n",
      "ITR 50000: loss_2=1.5099 | va_auroc:0.8614, va_auprc:0.4657\n"
     ]
    }
   ],
   "source": [
    "seed = 1234\n",
    "\n",
    "for out_itr in [1,2,3,4]:\n",
    "\n",
    "    if data_mode == 'CF':\n",
    "        tr_data_x,te_data_x, tr_data_y,te_data_y, tr_data_y_onehot,te_data_y_onehot = train_test_split(\n",
    "            data_x, data_y, data_y_onehot, test_size=0.2, random_state=seed+out_itr\n",
    "        )\n",
    "\n",
    "        tr_data_x,va_data_x, tr_data_y,va_data_y, tr_data_y_onehot,va_data_y_onehot = train_test_split(\n",
    "            tr_data_x, tr_data_y, tr_data_y_onehot, test_size=0.2, random_state=seed+out_itr\n",
    "        )\n",
    "    elif data_mode == 'CF_comorbidity':\n",
    "        tr_data_x,te_data_x, tr_data_y,te_data_y = train_test_split(\n",
    "            data_x, data_y, test_size=0.2, random_state=seed+out_itr\n",
    "        )\n",
    "\n",
    "        tr_data_x,va_data_x, tr_data_y,va_data_y = train_test_split(\n",
    "            tr_data_x, tr_data_y, test_size=0.2, random_state=seed+out_itr\n",
    "        )\n",
    "    elif data_mode == 'CF_comorbidity_select':\n",
    "        tr_data_x,te_data_x, tr_data_y,te_data_y = train_test_split(\n",
    "            data_x, data_y, test_size=0.2, random_state=seed+out_itr\n",
    "        )\n",
    "\n",
    "        tr_data_x,va_data_x, tr_data_y,va_data_y = train_test_split(\n",
    "            tr_data_x, tr_data_y, test_size=0.2, random_state=seed+out_itr\n",
    "        )\n",
    "\n",
    "    save_path = './{}/dcn_E2P/init/itr{}/'.format(data_mode, out_itr)\n",
    "\n",
    "    if not os.path.exists(save_path + '/models/'):\n",
    "        os.makedirs(save_path + '/models/')\n",
    "\n",
    "    if not os.path.exists(save_path + '/results/'):\n",
    "        os.makedirs(save_path + '/results/')\n",
    "\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    lr_rate    = 1e-3\n",
    "    keep_prob  = 0.7\n",
    "    mb_size    = 128\n",
    "\n",
    "\n",
    "    ITERATION  = 50000\n",
    "    check_step = 1000\n",
    "\n",
    "    avg_loss_mle  = 0\n",
    "    for itr in range(ITERATION):\n",
    "        x_mb, y_mb = f_get_minibatch(mb_size, tr_data_x, tr_data_y)\n",
    "\n",
    "        _, tmp_loss_mle= model.train_mle(x_mb, y_mb, lr_rate, keep_prob)\n",
    "        avg_loss_mle += tmp_loss_mle/check_step\n",
    "\n",
    "        if (itr+1)%check_step == 0:                \n",
    "            tmp_y, tmp_m = model.predict_y_hats(va_data_x)\n",
    "\n",
    "            y_pred = tmp_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
    "            y_true = va_data_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
    "\n",
    "            AUROC = np.zeros([y_dim])\n",
    "            AUPRC = np.zeros([y_dim])\n",
    "            for y_idx in range(y_dim):\n",
    "                auroc, auprc = f_get_prediction_scores(y_true[:, y_idx], y_pred[:, y_idx])\n",
    "                AUROC[y_idx] = auroc\n",
    "                AUPRC[y_idx] = auprc\n",
    "\n",
    "            print (\"ITR {}: loss_2={:.4f} | va_auroc:{:.4f}, va_auprc:{:.4f}\".format(\n",
    "                itr+1, avg_loss_mle, np.mean(AUROC), np.mean(AUPRC)))\n",
    "\n",
    "            avg_loss_mle = 0\n",
    "\n",
    "    saver.save(sess, save_path + 'models/dcn_E2P_v4_init')\n",
    "    save_logging(network_settings, save_path + 'models/network_settings_v4.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    For quantifying Purity, NMI, RI -> 8 true classes!\\n        - next_ABPA\\n        - next_Diabetes\\n        - next_Intestinal Obstruction\\n        \\n    For qualitative results\\n        - First, use all the available commorbidities!\\n'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    For quantifying Purity, NMI, RI -> 8 true classes!\n",
    "        - next_ABPA\n",
    "        - next_Diabetes\n",
    "        - next_Intestinal Obstruction\n",
    "        \n",
    "    For qualitative results\n",
    "        - First, use all the available commorbidities!\n",
    "'''# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8727475  0.77051879 0.88274351 0.81488763 0.87738161 0.89441366\n",
      " 0.89372434 0.93625356]\n",
      "[0.87966504 0.29058946 0.71388751 0.23338525 0.52075964 0.34831282\n",
      " 0.39325973 0.06607209]\n"
     ]
    }
   ],
   "source": [
    "tmp_y, tmp_m = model.predict_y_hats(te_data_x)\n",
    "\n",
    "y_pred = tmp_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
    "y_true = te_data_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
    "\n",
    "AUROC = np.zeros([y_dim])\n",
    "AUPRC = np.zeros([y_dim])\n",
    "for y_idx in range(y_dim):\n",
    "    auroc, auprc = f_get_prediction_scores(y_true[:, y_idx], y_pred[:, y_idx])\n",
    "    AUROC[y_idx] = auroc\n",
    "    AUPRC[y_idx] = auprc\n",
    "\n",
    "# df_result1 = pd.DataFrame(np.asarray([AUROC, AUPRC]).reshape([-1,2]),\n",
    "#              columns=['AUROC', 'AURPC'], index=label_list)\n",
    "# df_result1\n",
    "\n",
    "print(AUROC)\n",
    "print(AUPRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = './{}/dcn_E2P/init/itr{}/'.format(data_mode, out_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./CF_comorbidity_select/dcn_E2P/init/itr0/models/dcn_E2P_v4_init\n"
     ]
    }
   ],
   "source": [
    "input_dims ={\n",
    "    'x_dim': x_dim,\n",
    "    'y_dim': y_dim,\n",
    "    'y_type': y_type,\n",
    "    'max_length': max_length    \n",
    "}\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Turn on xla optimization\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "network_settings = load_logging(load_path + 'models/network_settings_v4.txt')\n",
    "\n",
    "model = DCN_E2P(sess, \"dcn_E2P\", input_dims, network_settings)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver.restore(sess, load_path + 'models/dcn_E2P_v4_init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "K          = 8\n",
    "# for num_Cluster in NUM_CLUSTER:\n",
    "lr_rate    = 1e-4\n",
    "keep_prob  = 0.7\n",
    "mb_size    = 128\n",
    "\n",
    "alpha      = 0.1  #L_CLUSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLUSTER INITIALIZATION\n",
    "num_Cluster = K\n",
    "\n",
    "km   = MiniBatchKMeans(n_clusters = num_Cluster,  batch_size=mb_size)\n",
    "tr_z = model.predict_Z(tr_data_x)\n",
    "\n",
    "_    = km.fit(tr_z)\n",
    "mu   = km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changhee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: Explicit initial center position passed: performing only one init in MiniBatchKMeans instead of n_init=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITR 100: loss_total=2.1159\t loss_mle=1.4786 \t loss_clue=6.3726\n",
      "ITR 200: loss_total=2.1269\t loss_mle=1.5113 \t loss_clue=6.1558\n",
      "ITR 300: loss_total=2.1182\t loss_mle=1.5151 \t loss_clue=6.0307\n",
      "ITR 400: loss_total=2.0631\t loss_mle=1.4759 \t loss_clue=5.8722\n",
      "ITR 500: loss_total=2.0529\t loss_mle=1.4732 \t loss_clue=5.7971\n",
      "ITR 600: loss_total=2.0309\t loss_mle=1.4601 \t loss_clue=5.7072\n",
      "ITR 700: loss_total=2.0790\t loss_mle=1.5206 \t loss_clue=5.5837\n",
      "ITR 800: loss_total=2.0520\t loss_mle=1.4982 \t loss_clue=5.5378\n",
      "ITR 900: loss_total=2.0384\t loss_mle=1.4891 \t loss_clue=5.4933\n",
      "ITR 1000: loss_total=2.0119\t loss_mle=1.4703 \t loss_clue=5.4158\n",
      "ITR 1100: loss_total=1.9909\t loss_mle=1.4541 \t loss_clue=5.3688\n",
      "ITR 1200: loss_total=2.0245\t loss_mle=1.4945 \t loss_clue=5.2999\n",
      "ITR 1300: loss_total=2.0368\t loss_mle=1.5097 \t loss_clue=5.2710\n",
      "ITR 1400: loss_total=1.9674\t loss_mle=1.4486 \t loss_clue=5.1879\n",
      "ITR 1500: loss_total=1.9949\t loss_mle=1.4766 \t loss_clue=5.1829\n",
      "ITR 1600: loss_total=2.0017\t loss_mle=1.4904 \t loss_clue=5.1133\n",
      "ITR 1700: loss_total=2.0309\t loss_mle=1.5202 \t loss_clue=5.1062\n",
      "ITR 1800: loss_total=1.9939\t loss_mle=1.4879 \t loss_clue=5.0603\n",
      "ITR 1900: loss_total=1.9895\t loss_mle=1.4891 \t loss_clue=5.0045\n",
      "ITR 2000: loss_total=2.0161\t loss_mle=1.5181 \t loss_clue=4.9803\n",
      "ITR 2100: loss_total=1.9830\t loss_mle=1.4865 \t loss_clue=4.9649\n",
      "ITR 2200: loss_total=1.9886\t loss_mle=1.4941 \t loss_clue=4.9453\n",
      "ITR 2300: loss_total=1.9677\t loss_mle=1.4761 \t loss_clue=4.9160\n",
      "ITR 2400: loss_total=1.9932\t loss_mle=1.5052 \t loss_clue=4.8800\n",
      "ITR 2500: loss_total=1.9789\t loss_mle=1.4967 \t loss_clue=4.8218\n",
      "ITR 2600: loss_total=1.9716\t loss_mle=1.4932 \t loss_clue=4.7850\n",
      "ITR 2700: loss_total=1.9509\t loss_mle=1.4737 \t loss_clue=4.7718\n",
      "ITR 2800: loss_total=1.9370\t loss_mle=1.4620 \t loss_clue=4.7496\n",
      "ITR 2900: loss_total=1.9651\t loss_mle=1.4888 \t loss_clue=4.7632\n",
      "ITR 3000: loss_total=1.9536\t loss_mle=1.4831 \t loss_clue=4.7054\n",
      "ITR 3100: loss_total=1.9619\t loss_mle=1.4882 \t loss_clue=4.7366\n",
      "ITR 3200: loss_total=1.9625\t loss_mle=1.4897 \t loss_clue=4.7282\n",
      "ITR 3300: loss_total=1.9826\t loss_mle=1.5147 \t loss_clue=4.6785\n",
      "ITR 3400: loss_total=1.9299\t loss_mle=1.4662 \t loss_clue=4.6367\n",
      "ITR 3500: loss_total=1.9344\t loss_mle=1.4714 \t loss_clue=4.6305\n",
      "ITR 3600: loss_total=1.9343\t loss_mle=1.4724 \t loss_clue=4.6194\n",
      "ITR 3700: loss_total=1.9405\t loss_mle=1.4822 \t loss_clue=4.5824\n",
      "ITR 3800: loss_total=1.9323\t loss_mle=1.4762 \t loss_clue=4.5618\n",
      "ITR 3900: loss_total=1.9237\t loss_mle=1.4698 \t loss_clue=4.5394\n",
      "ITR 4000: loss_total=1.9420\t loss_mle=1.4903 \t loss_clue=4.5170\n",
      "ITR 4100: loss_total=1.9494\t loss_mle=1.4969 \t loss_clue=4.5247\n",
      "ITR 4200: loss_total=1.9583\t loss_mle=1.5074 \t loss_clue=4.5090\n",
      "ITR 4300: loss_total=1.9415\t loss_mle=1.4929 \t loss_clue=4.4861\n",
      "ITR 4400: loss_total=1.9351\t loss_mle=1.4872 \t loss_clue=4.4790\n",
      "ITR 4500: loss_total=1.9183\t loss_mle=1.4708 \t loss_clue=4.4758\n",
      "ITR 4600: loss_total=1.8838\t loss_mle=1.4405 \t loss_clue=4.4326\n",
      "ITR 4700: loss_total=1.8888\t loss_mle=1.4468 \t loss_clue=4.4206\n",
      "ITR 4800: loss_total=1.9066\t loss_mle=1.4693 \t loss_clue=4.3727\n",
      "ITR 4900: loss_total=1.8891\t loss_mle=1.4505 \t loss_clue=4.3863\n",
      "ITR 5000: loss_total=1.8915\t loss_mle=1.4524 \t loss_clue=4.3917\n",
      "ITR 5100: loss_total=1.9144\t loss_mle=1.4779 \t loss_clue=4.3648\n",
      "ITR 5200: loss_total=1.9270\t loss_mle=1.4962 \t loss_clue=4.3078\n",
      "ITR 5300: loss_total=1.9191\t loss_mle=1.4854 \t loss_clue=4.3377\n",
      "ITR 5400: loss_total=1.8822\t loss_mle=1.4515 \t loss_clue=4.3071\n",
      "ITR 5500: loss_total=1.8962\t loss_mle=1.4656 \t loss_clue=4.3058\n",
      "ITR 5600: loss_total=1.9037\t loss_mle=1.4782 \t loss_clue=4.2548\n",
      "ITR 5700: loss_total=1.9377\t loss_mle=1.5139 \t loss_clue=4.2385\n",
      "ITR 5800: loss_total=1.9240\t loss_mle=1.4971 \t loss_clue=4.2698\n",
      "ITR 5900: loss_total=1.8771\t loss_mle=1.4491 \t loss_clue=4.2801\n",
      "ITR 6000: loss_total=1.9000\t loss_mle=1.4772 \t loss_clue=4.2277\n",
      "ITR 6100: loss_total=1.8962\t loss_mle=1.4763 \t loss_clue=4.1988\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-603991959214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0ms_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     _, tmp_loss_total, tmp_loss_mle, tmp_loss_clu = model.train_total(\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_Cluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     )     \n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-fd81ba21c6b9>\u001b[0m in \u001b[0;36mtrain_total\u001b[0;34m(self, x_, y_, s_, E_, K_, a, lr_train, k_prob)\u001b[0m\n\u001b[1;32m    211\u001b[0m                              feed_dict={self.x: x_, self.y: y_, self.s: s_, self.E: E_, self.K: K_,\n\u001b[1;32m    212\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                         self.lr_rate: lr_train, self.keep_prob: k_prob})\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_y_hats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ITERATION = 10000\n",
    "\n",
    "check_step = 100\n",
    "\n",
    "avg_loss_total = 0\n",
    "avg_loss_mle   = 0\n",
    "avg_loss_clu   = 0\n",
    "\n",
    "for itr in range(ITERATION):\n",
    "    x_mb, y_mb = f_get_minibatch(mb_size, tr_data_x, tr_data_y)\n",
    "    \n",
    "    s_mb = km.predict(model.predict_Z(x_mb))\n",
    "    _, tmp_loss_total, tmp_loss_mle, tmp_loss_clu = model.train_total(\n",
    "        x_mb, y_mb, s_mb, mu, num_Cluster, alpha, lr_rate, keep_prob\n",
    "    )     \n",
    "        \n",
    "    avg_loss_total += tmp_loss_total/check_step\n",
    "    avg_loss_mle += tmp_loss_mle/check_step\n",
    "    avg_loss_clu += tmp_loss_clu/check_step\n",
    "    \n",
    "    \n",
    "    if (itr+1)%check_step == 0:\n",
    "        km   = MiniBatchKMeans(n_clusters = num_Cluster,  batch_size=mb_size, init=mu)\n",
    "        tr_z = model.predict_Z(tr_data_x)\n",
    "\n",
    "        _    = km.fit(tr_z)\n",
    "        mu   = km.cluster_centers_        \n",
    "        \n",
    "        print (\"ITR {}: loss_total={:.4f}\\t loss_mle={:.4f} \\t loss_clue={:.4f}\".format(\n",
    "            itr+1, avg_loss_total, avg_loss_mle, avg_loss_clu))\n",
    "        avg_loss_total = 0\n",
    "        avg_loss_mle   = 0\n",
    "        avg_loss_clu   = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './{}/dcn_E2P/K{}/itr{}/'.format(data_mode, K, out_itr)\n",
    "\n",
    "if not os.path.exists(save_path + '/models/'):\n",
    "    os.makedirs(save_path + '/models/')\n",
    "\n",
    "if not os.path.exists(save_path + '/results/'):\n",
    "    os.makedirs(save_path + '/results/')\n",
    "    \n",
    "saver.save(sess, save_path + 'models/dcn_E2P_clustered_v3')\n",
    "\n",
    "save_logging(network_settings, save_path + 'models/network_settings.txt')\n",
    "np.savez(save_path + 'models/embeddings.npz', km=km, mu=mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_z, tmp_m = model.predict_zs(te_data_x)\n",
    "\n",
    "tmp_z  = tmp_z.reshape([-1, z_dim])[tmp_m.reshape([-1]) == 1]\n",
    "pred_y = km.predict(tmp_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "yyy = sess.run(model.yy, feed_dict={model.zz:km.cluster_centers_, model.mb_size:num_Cluster, model.keep_prob:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.9901032e-01, 9.9985099e-01, 7.3584542e-04, 4.2787267e-04,\n",
       "       9.9994671e-01, 9.9652326e-01, 5.7657436e-03, 3.6333574e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yyy[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITR0 - K8 |  NMI:0.2823, RI:0.1859, PURITY:0.7359\n"
     ]
    }
   ],
   "source": [
    "tmp_z, tmp_m = model.predict_zs(te_data_x)\n",
    "\n",
    "tmp_z  = tmp_z.reshape([-1, z_dim])[tmp_m.reshape([-1]) == 1]\n",
    "pred_y = km.predict(tmp_z)\n",
    "\n",
    "true_y = (te_data_y * np.tile(np.expand_dims(tmp_m, axis=2), [1,1,y_dim])).reshape([-1, y_dim])\n",
    "true_y = true_y[(tmp_m.reshape([-1]) == 1)]\n",
    "true_y = np.argmax(true_y, axis=1)\n",
    "\n",
    "tmp_nmi    = normalized_mutual_info_score(true_y, pred_y)\n",
    "tmp_ri     = adjusted_rand_score(true_y, pred_y)\n",
    "tmp_purity = purity_score(true_y, pred_y)\n",
    "\n",
    "# pd.DataFrame([[tmp_nmi, tmp_ri, tmp_purity]], \n",
    "#              columns=['NMI', 'RI', 'PURITY'], \n",
    "#              index=['itr'+str(out_itr)]).to_csv(save_path + 'results/nmi_ir_purity.csv')\n",
    "\n",
    "print('ITR{} - K{} |  NMI:{:.4f}, RI:{:.4f}, PURITY:{:.4f}'.format(out_itr, K, tmp_nmi, tmp_ri, tmp_purity))\n",
    "\n",
    "RESULT_NMI[out_itr, 0]    = tmp_nmi\n",
    "RESULT_RI[out_itr, 0]     = tmp_ri\n",
    "RESULT_PURITY[out_itr, 0] = tmp_purity\n",
    "\n",
    "\n",
    "# pd.DataFrame(RESULT_NMI, \n",
    "#              columns=['NMI'], \n",
    "#              index=['itr'+str(out_itr) for out_itr in range(OUT_ITERATION)]).to_csv('./{}/dcn_E2P/K{}/'.format(data_mode, K) + 'results_nmi.csv')\n",
    "\n",
    "# pd.DataFrame(RESULT_RI, \n",
    "#              columns=['RI'], \n",
    "#              index=['itr'+str(out_itr) for out_itr in range(OUT_ITERATION)]).to_csv('./{}/dcn_E2P/K{}/'.format(data_mode, K) + 'results_ri.csv')\n",
    "\n",
    "# pd.DataFrame(RESULT_PURITY, \n",
    "#              columns=['PURITY'], \n",
    "#              index=['itr'+str(out_itr) for out_itr in range(OUT_ITERATION)]).to_csv('./{}/dcn_E2P/K{}/'.format(data_mode, K) + 'results_purity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KMEANS:\n",
    "def get_predictions_for_kmeans(x_):\n",
    "    tmp_z, _      = model.predict_zs(x_)\n",
    "    tmp_y, tmp_m  = model.predict_y_hats(x_)\n",
    "    return tmp_z, tmp_y, tmp_m\n",
    "\n",
    "def get_kmeans(input_, K_):\n",
    "    km_    = KMeans(n_clusters = K_, init='k-means++')\n",
    "    _      = km_.fit(input_)\n",
    "    tmp_c  = km_.cluster_centers_\n",
    "    return km_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATION = 5\n",
    "NUM_CLUSTERS = [4, 8, 16]\n",
    "\n",
    "RESULTS_Z_NMI    = np.zeros([ITERATION, len(NUM_CLUSTERS)])\n",
    "RESULTS_Z_PURITY = np.zeros([ITERATION, len(NUM_CLUSTERS)])\n",
    "RESULTS_Z_RI     = np.zeros([ITERATION, len(NUM_CLUSTERS)])\n",
    "\n",
    "RESULTS_Y_NMI    = np.zeros([ITERATION, len(NUM_CLUSTERS)])\n",
    "RESULTS_Y_PURITY = np.zeros([ITERATION, len(NUM_CLUSTERS)])\n",
    "RESULTS_Y_RI     = np.zeros([ITERATION, len(NUM_CLUSTERS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./CF_comorbidity_select/dcn_E2P/init/itr0/models/dcn_E2P_v4_init\n",
      "ITR0 - K4 |  NMI:0.2773, RI:0.2577, PURITY:0.7228\n",
      "ITR0 - K8 |  NMI:0.2092, RI:0.1036, PURITY:0.7108\n",
      "ITR0 - K16 |  NMI:0.2062, RI:0.0611, PURITY:0.7158\n",
      "ITR0 - K4 |  NMI:0.3256, RI:0.4854, PURITY:0.7196\n",
      "ITR0 - K8 |  NMI:0.3201, RI:0.4327, PURITY:0.7443\n",
      "ITR0 - K16 |  NMI:0.2894, RI:0.3195, PURITY:0.7477\n",
      "INFO:tensorflow:Restoring parameters from ./CF_comorbidity_select/dcn_E2P/init/itr1/models/dcn_E2P_v4_init\n",
      "ITR1 - K4 |  NMI:0.1913, RI:0.1496, PURITY:0.7146\n",
      "ITR1 - K8 |  NMI:0.1961, RI:0.1045, PURITY:0.7179\n",
      "ITR1 - K16 |  NMI:0.2002, RI:0.0626, PURITY:0.7268\n",
      "ITR1 - K4 |  NMI:0.3298, RI:0.4863, PURITY:0.7385\n",
      "ITR1 - K8 |  NMI:0.3274, RI:0.4495, PURITY:0.7565\n",
      "ITR1 - K16 |  NMI:0.2943, RI:0.3310, PURITY:0.7644\n",
      "INFO:tensorflow:Restoring parameters from ./CF_comorbidity_select/dcn_E2P/init/itr2/models/dcn_E2P_v4_init\n",
      "ITR2 - K4 |  NMI:0.2727, RI:0.2459, PURITY:0.7261\n",
      "ITR2 - K8 |  NMI:0.2351, RI:0.1217, PURITY:0.7368\n",
      "ITR2 - K16 |  NMI:0.2405, RI:0.0812, PURITY:0.7574\n",
      "ITR2 - K4 |  NMI:0.3475, RI:0.5120, PURITY:0.7460\n",
      "ITR2 - K8 |  NMI:0.3470, RI:0.4697, PURITY:0.7678\n",
      "ITR2 - K16 |  NMI:0.3041, RI:0.3313, PURITY:0.7690\n",
      "INFO:tensorflow:Restoring parameters from ./CF_comorbidity_select/dcn_E2P/init/itr3/models/dcn_E2P_v4_init\n",
      "ITR3 - K4 |  NMI:0.2104, RI:0.1484, PURITY:0.7145\n",
      "ITR3 - K8 |  NMI:0.2065, RI:0.1022, PURITY:0.7147\n",
      "ITR3 - K16 |  NMI:0.2187, RI:0.0654, PURITY:0.7293\n",
      "ITR3 - K4 |  NMI:0.3068, RI:0.4636, PURITY:0.7143\n",
      "ITR3 - K8 |  NMI:0.3094, RI:0.4181, PURITY:0.7394\n",
      "ITR3 - K16 |  NMI:0.2764, RI:0.3124, PURITY:0.7404\n",
      "INFO:tensorflow:Restoring parameters from ./CF_comorbidity_select/dcn_E2P/init/itr4/models/dcn_E2P_v4_init\n",
      "ITR4 - K4 |  NMI:0.1862, RI:0.1490, PURITY:0.7029\n",
      "ITR4 - K8 |  NMI:0.2093, RI:0.1022, PURITY:0.7163\n",
      "ITR4 - K16 |  NMI:0.2123, RI:0.0680, PURITY:0.7298\n",
      "ITR4 - K4 |  NMI:0.3135, RI:0.4754, PURITY:0.7220\n",
      "ITR4 - K8 |  NMI:0.3209, RI:0.4303, PURITY:0.7490\n",
      "ITR4 - K16 |  NMI:0.2879, RI:0.3056, PURITY:0.7505\n"
     ]
    }
   ],
   "source": [
    "seed = 1234\n",
    "\n",
    "for out_itr in [0,1,2,3,4]:\n",
    "\n",
    "    if data_mode == 'CF':\n",
    "        tr_data_x,te_data_x, tr_data_y,te_data_y, tr_data_y_onehot,te_data_y_onehot = train_test_split(\n",
    "            data_x, data_y, data_y_onehot, test_size=0.2, random_state=seed+out_itr\n",
    "        )\n",
    "\n",
    "        tr_data_x,va_data_x, tr_data_y,va_data_y, tr_data_y_onehot,va_data_y_onehot = train_test_split(\n",
    "            tr_data_x, tr_data_y, tr_data_y_onehot, test_size=0.2, random_state=seed+out_itr\n",
    "        )\n",
    "    elif data_mode == 'CF_comorbidity':\n",
    "        tr_data_x,te_data_x, tr_data_y,te_data_y = train_test_split(\n",
    "            data_x, data_y, test_size=0.2, random_state=seed+out_itr\n",
    "        )\n",
    "\n",
    "        tr_data_x,va_data_x, tr_data_y,va_data_y = train_test_split(\n",
    "            tr_data_x, tr_data_y, test_size=0.2, random_state=seed+out_itr\n",
    "        )\n",
    "    elif data_mode == 'CF_comorbidity_select':\n",
    "        tr_data_x,te_data_x, tr_data_y,te_data_y = train_test_split(\n",
    "            data_x, data_y, test_size=0.2, random_state=seed+out_itr\n",
    "        )\n",
    "\n",
    "        tr_data_x,va_data_x, tr_data_y,va_data_y = train_test_split(\n",
    "            tr_data_x, tr_data_y, test_size=0.2, random_state=seed+out_itr\n",
    "        )\n",
    "\n",
    "\n",
    "    load_path = './{}/dcn_E2P/init/itr{}/'.format(data_mode, out_itr)\n",
    "\n",
    "    input_dims ={\n",
    "        'x_dim': x_dim,\n",
    "        'y_dim': y_dim,\n",
    "        'y_type': y_type,\n",
    "        'max_length': max_length    \n",
    "    }\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Turn on xla optimization\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    network_settings = load_logging(load_path + 'models/network_settings_v4.txt')\n",
    "\n",
    "    model = DCN_E2P(sess, \"dcn_E2P\", input_dims, network_settings)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver.restore(sess, load_path + 'models/dcn_E2P_v4_init')\n",
    "\n",
    "\n",
    "    for c_idx, K_kmeans in enumerate(NUM_CLUSTERS):\n",
    "        tmp_z, tmp_y, tmp_m = get_predictions_for_kmeans(tr_data_x)\n",
    "        tmp_z               = tmp_z.reshape([-1, z_dim])[tmp_m.reshape([-1]) == 1]\n",
    "\n",
    "        km = get_kmeans(tmp_z, K_kmeans)\n",
    "\n",
    "        tmp_z, tmp_y, tmp_m = get_predictions_for_kmeans(te_data_x)\n",
    "\n",
    "        pred_y = km.predict(tmp_z.reshape([-1, z_dim])[tmp_m.reshape([-1]) == 1])\n",
    "\n",
    "        true_y = (te_data_y * np.tile(np.expand_dims(tmp_m, axis=2), [1,1,y_dim])).reshape([-1, y_dim])\n",
    "        true_y = true_y[(tmp_m.reshape([-1]) == 1)]\n",
    "        true_y = np.argmax(true_y, axis=1)\n",
    "\n",
    "        tmp_nmi    = normalized_mutual_info_score(true_y, pred_y)\n",
    "        tmp_ri     = adjusted_rand_score(true_y, pred_y)\n",
    "        tmp_purity = purity_score(true_y, pred_y)\n",
    "\n",
    "        print('ITR{} - K{} |  NMI:{:.4f}, RI:{:.4f}, PURITY:{:.4f}'.format(out_itr, K_kmeans, tmp_nmi, tmp_ri, tmp_purity))\n",
    "\n",
    "        RESULTS_Z_NMI[out_itr, c_idx]    = tmp_nmi\n",
    "        RESULTS_Z_RI[out_itr, c_idx]     = tmp_ri\n",
    "        RESULTS_Z_PURITY[out_itr, c_idx] = tmp_purity\n",
    "\n",
    "\n",
    "    for c_idx, K_kmeans in enumerate(NUM_CLUSTERS):\n",
    "        tmp_z, tmp_y, tmp_m = get_predictions_for_kmeans(tr_data_x)\n",
    "        tmp_y               = tmp_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1]\n",
    "\n",
    "        km = get_kmeans(tmp_y, K_kmeans)\n",
    "\n",
    "        tmp_z, tmp_y, tmp_m = get_predictions_for_kmeans(te_data_x)\n",
    "\n",
    "        pred_y = km.predict(tmp_y.reshape([-1, y_dim])[tmp_m.reshape([-1]) == 1])\n",
    "\n",
    "        true_y = (te_data_y * np.tile(np.expand_dims(tmp_m, axis=2), [1,1,y_dim])).reshape([-1, y_dim])\n",
    "        true_y = true_y[(tmp_m.reshape([-1]) == 1)]\n",
    "        true_y = np.argmax(true_y, axis=1)\n",
    "\n",
    "        tmp_nmi    = normalized_mutual_info_score(true_y, pred_y)\n",
    "        tmp_ri     = adjusted_rand_score(true_y, pred_y)\n",
    "        tmp_purity = purity_score(true_y, pred_y)\n",
    "\n",
    "        print('ITR{} - K{} |  NMI:{:.4f}, RI:{:.4f}, PURITY:{:.4f}'.format(out_itr, K_kmeans, tmp_nmi, tmp_ri, tmp_purity))\n",
    "\n",
    "        RESULTS_Y_NMI[out_itr, c_idx]    = tmp_nmi\n",
    "        RESULTS_Y_RI[out_itr, c_idx]     = tmp_ri\n",
    "        RESULTS_Y_PURITY[out_itr, c_idx] = tmp_purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_z = './{}/RESULTS/kmeans_z/'.format(data_mode)\n",
    "save_path_y = './{}/RESULTS/kmeans_y/'.format(data_mode)\n",
    "\n",
    "if not os.path.exists(save_path_z):\n",
    "    os.makedirs(save_path_z)\n",
    "\n",
    "if not os.path.exists(save_path_y):\n",
    "    os.makedirs(save_path_y)\n",
    "    \n",
    "    \n",
    "tmp_column = ['K'+str(k) for k in NUM_CLUSTERS]\n",
    "tmp_index  = ['itr'+str(out_itr) for out_itr in range(ITERATION)]\n",
    "\n",
    "pd.DataFrame(RESULTS_Z_NMI, columns=tmp_column, index=tmp_index).to_csv(save_path_z + 'results_nmi.csv')\n",
    "pd.DataFrame(RESULTS_Z_RI, columns=tmp_column, index=tmp_index).to_csv(save_path_z + 'results_ri.csv')\n",
    "pd.DataFrame(RESULTS_Z_PURITY, columns=tmp_column, index=tmp_index).to_csv(save_path_z + 'results_purity.csv')\n",
    "\n",
    "pd.DataFrame(RESULTS_Y_NMI, columns=tmp_column, index=tmp_index).to_csv(save_path_y + 'results_nmi.csv')\n",
    "pd.DataFrame(RESULTS_Y_RI, columns=tmp_column, index=tmp_index).to_csv(save_path_y + 'results_ri.csv')\n",
    "pd.DataFrame(RESULTS_Y_PURITY, columns=tmp_column, index=tmp_index).to_csv(save_path_y + 'results_purity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
